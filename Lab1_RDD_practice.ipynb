{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8b3ba45-9b0b-43f2-9c84-2ce8e7aec6ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "For help, look here:\n",
    "https://spark.apache.org/docs/latest/rdd-programming-guide.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0145c6ca-d9f7-4bd7-8309-3fee0a69c3b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/databricks-datasets/COVID/</td><td>COVID/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/README.md</td><td>README.md</td><td>976</td><td>1532468253000</td></tr><tr><td>dbfs:/databricks-datasets/Rdatasets/</td><td>Rdatasets/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/SPARK_README.md</td><td>SPARK_README.md</td><td>3359</td><td>1455043490000</td></tr><tr><td>dbfs:/databricks-datasets/adult/</td><td>adult/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/airlines/</td><td>airlines/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/amazon/</td><td>amazon/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/asa/</td><td>asa/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/atlas_higgs/</td><td>atlas_higgs/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/bikeSharing/</td><td>bikeSharing/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/cctvVideos/</td><td>cctvVideos/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/credit-card-fraud/</td><td>credit-card-fraud/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/cs100/</td><td>cs100/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/cs110x/</td><td>cs110x/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/cs190/</td><td>cs190/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/data.gov/</td><td>data.gov/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/definitive-guide/</td><td>definitive-guide/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/delta-sharing/</td><td>delta-sharing/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/flights/</td><td>flights/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/flower_photos/</td><td>flower_photos/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/flowers/</td><td>flowers/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/genomics/</td><td>genomics/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/hail/</td><td>hail/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/identifying-campaign-effectiveness/</td><td>identifying-campaign-effectiveness/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/iot/</td><td>iot/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/iot-stream/</td><td>iot-stream/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/learning-spark/</td><td>learning-spark/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/learning-spark-v2/</td><td>learning-spark-v2/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/lending-club-loan-stats/</td><td>lending-club-loan-stats/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/med-images/</td><td>med-images/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/media/</td><td>media/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/mnist-digits/</td><td>mnist-digits/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/news20.binary/</td><td>news20.binary/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/nyctaxi/</td><td>nyctaxi/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/nyctaxi-with-zipcodes/</td><td>nyctaxi-with-zipcodes/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/online_retail/</td><td>online_retail/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/overlap-join/</td><td>overlap-join/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/power-plant/</td><td>power-plant/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/retail-org/</td><td>retail-org/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/rwe/</td><td>rwe/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/sai-summit-2019-sf/</td><td>sai-summit-2019-sf/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/sample_logs/</td><td>sample_logs/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/samples/</td><td>samples/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/sfo_customer_survey/</td><td>sfo_customer_survey/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/sms_spam_collection/</td><td>sms_spam_collection/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/songs/</td><td>songs/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/</td><td>structured-streaming/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/timeseries/</td><td>timeseries/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/tpch/</td><td>tpch/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/travel_recommendations_realtime/</td><td>travel_recommendations_realtime/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/warmup/</td><td>warmup/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/weather/</td><td>weather/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/wiki/</td><td>wiki/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/wikipedia-datasets/</td><td>wikipedia-datasets/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/wine-quality/</td><td>wine-quality/</td><td>0</td><td>0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/databricks-datasets/COVID/",
         "COVID/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/README.md",
         "README.md",
         976,
         1532468253000
        ],
        [
         "dbfs:/databricks-datasets/Rdatasets/",
         "Rdatasets/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/SPARK_README.md",
         "SPARK_README.md",
         3359,
         1455043490000
        ],
        [
         "dbfs:/databricks-datasets/adult/",
         "adult/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/airlines/",
         "airlines/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/amazon/",
         "amazon/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/asa/",
         "asa/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/atlas_higgs/",
         "atlas_higgs/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/bikeSharing/",
         "bikeSharing/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/cctvVideos/",
         "cctvVideos/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/credit-card-fraud/",
         "credit-card-fraud/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/cs100/",
         "cs100/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/cs110x/",
         "cs110x/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/cs190/",
         "cs190/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/data.gov/",
         "data.gov/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/definitive-guide/",
         "definitive-guide/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/delta-sharing/",
         "delta-sharing/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/flights/",
         "flights/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/flower_photos/",
         "flower_photos/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/flowers/",
         "flowers/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/genomics/",
         "genomics/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/hail/",
         "hail/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/identifying-campaign-effectiveness/",
         "identifying-campaign-effectiveness/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/iot/",
         "iot/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/iot-stream/",
         "iot-stream/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/learning-spark/",
         "learning-spark/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/learning-spark-v2/",
         "learning-spark-v2/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/lending-club-loan-stats/",
         "lending-club-loan-stats/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/med-images/",
         "med-images/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/media/",
         "media/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/mnist-digits/",
         "mnist-digits/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/news20.binary/",
         "news20.binary/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/nyctaxi/",
         "nyctaxi/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/nyctaxi-with-zipcodes/",
         "nyctaxi-with-zipcodes/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/online_retail/",
         "online_retail/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/overlap-join/",
         "overlap-join/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/power-plant/",
         "power-plant/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/retail-org/",
         "retail-org/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/rwe/",
         "rwe/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/sai-summit-2019-sf/",
         "sai-summit-2019-sf/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/sample_logs/",
         "sample_logs/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/samples/",
         "samples/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/sfo_customer_survey/",
         "sfo_customer_survey/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/sms_spam_collection/",
         "sms_spam_collection/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/songs/",
         "songs/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/structured-streaming/",
         "structured-streaming/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/timeseries/",
         "timeseries/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/tpch/",
         "tpch/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/travel_recommendations_realtime/",
         "travel_recommendations_realtime/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/warmup/",
         "warmup/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/weather/",
         "weather/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/wiki/",
         "wiki/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/wikipedia-datasets/",
         "wikipedia-datasets/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/wine-quality/",
         "wine-quality/",
         0,
         0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check out the pre-loaded dataset\n",
    "display(dbutils.fs.ls('dbfs:/databricks-datasets/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "195d300a-63f0-4dc8-b0c4-bc1c4cf6a9b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51aa0658-e68f-4a62-8962-5b1baa0ede76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a rdd (sc = SparkContext)\n",
    "rdd = sc.textFile(\"dbfs:/databricks-datasets/SPARK_README.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eaf066b0-5738-4d2d-a225-8ae0d44d3a78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[3]: ['# Apache Spark',\n '',\n 'Spark is a fast and general cluster computing system for Big Data. It provides',\n 'high-level APIs in Scala, Java, Python, and R, and an optimized engine that',\n 'supports general computation graphs for data analysis. It also supports a',\n 'rich set of higher-level tools including Spark SQL for SQL and DataFrames,',\n 'MLlib for machine learning, GraphX for graph processing,',\n 'and Spark Streaming for stream processing.',\n '',\n '<http://spark.apache.org/>',\n '',\n '',\n '## Online Documentation',\n '',\n 'You can find the latest Spark documentation, including a programming',\n 'guide, on the [project web page](http://spark.apache.org/documentation.html)',\n 'and [project wiki](https://cwiki.apache.org/confluence/display/SPARK).',\n 'This README file only contains basic setup instructions.',\n '',\n '## Building Spark']"
     ]
    }
   ],
   "source": [
    "# Read 20 lines \n",
    "rdd.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7c90957-5053-4d75-8930-9facef942a69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#\nApache\nSpark\n\nSpark\nis\na\nfast\nand\ngeneral\ncluster\ncomputing\nsystem\nfor\nBig\nData.\nIt\nprovides\nhigh-level\nAPIs\nin\nScala,\nJava,\nPython,\nand\nR,\nand\nan\noptimized\nengine\nthat\nsupports\ngeneral\ncomputation\ngraphs\nfor\ndata\nanalysis.\nIt\nalso\nsupports\na\nrich\nset\nof\nhigher-level\ntools\nincluding\nSpark\nSQL\nfor\nSQL\nand\nDataFrames,\nMLlib\nfor\nmachine\nlearning,\nGraphX\nfor\ngraph\nprocessing,\nand\nSpark\nStreaming\nfor\nstream\nprocessing.\n\n<http://spark.apache.org/>\n\n\n##\nOnline\nDocumentation\n\nYou\ncan\nfind\nthe\nlatest\nSpark\ndocumentation,\nincluding\na\nprogramming\nguide,\non\nthe\n[project\nweb\npage](http://spark.apache.org/documentation.html)\nand\n[project\nwiki](https://cwiki.apache.org/confluence/display/SPARK).\nThis\nREADME\nfile\nonly\ncontains\nbasic\nsetup\ninstructions.\n\n##\nBuilding\nSpark\n\nSpark\nis\nbuilt\nusing\n[Apache\nMaven](http://maven.apache.org/).\nTo\nbuild\nSpark\nand\nits\nexample\nprograms,\nrun:\n\n\n\n\n\nbuild/mvn\n-DskipTests\nclean\npackage\n\n(You\ndo\nnot\nneed\nto\ndo\nthis\nif\nyou\ndownloaded\na\npre-built\npackage.)\nMore\ndetailed\ndocumentation\nis\navailable\nfrom\nthe\nproject\nsite,\nat\n[\"Building\nSpark\"](http://spark.apache.org/docs/latest/building-spark.html).\n\n##\nInteractive\nScala\nShell\n\nThe\neasiest\nway\nto\nstart\nusing\nSpark\nis\nthrough\nthe\nScala\nshell:\n\n\n\n\n\n./bin/spark-shell\n\nTry\nthe\nfollowing\ncommand,\nwhich\nshould\nreturn\n1000:\n\n\n\n\n\nscala>\nsc.parallelize(1\nto\n1000).count()\n\n##\nInteractive\nPython\nShell\n\nAlternatively,\nif\nyou\nprefer\nPython,\nyou\ncan\nuse\nthe\nPython\nshell:\n\n\n\n\n\n./bin/pyspark\n\nAnd\nrun\nthe\nfollowing\ncommand,\nwhich\nshould\nalso\nreturn\n1000:\n\n\n\n\n\n>>>\nsc.parallelize(range(1000)).count()\n\n##\nExample\nPrograms\n\nSpark\nalso\ncomes\nwith\nseveral\nsample\nprograms\nin\nthe\n`examples`\ndirectory.\nTo\nrun\none\nof\nthem,\nuse\n`./bin/run-example\n<class>\n[params]`.\nFor\nexample:\n\n\n\n\n\n./bin/run-example\nSparkPi\n\nwill\nrun\nthe\nPi\nexample\nlocally.\n\nYou\ncan\nset\nthe\nMASTER\nenvironment\nvariable\nwhen\nrunning\nexamples\nto\nsubmit\nexamples\nto\na\ncluster.\nThis\ncan\nbe\na\nmesos://\nor\nspark://\nURL,\n\"yarn\"\nto\nrun\non\nYARN,\nand\n\"local\"\nto\nrun\nlocally\nwith\none\nthread,\nor\n\"local[N]\"\nto\nrun\nlocally\nwith\nN\nthreads.\nYou\ncan\nalso\nuse\nan\nabbreviated\nclass\nname\nif\nthe\nclass\nis\nin\nthe\n`examples`\npackage.\nFor\ninstance:\n\n\n\n\n\nMASTER=spark://host:7077\n./bin/run-example\nSparkPi\n\nMany\nof\nthe\nexample\nprograms\nprint\nusage\nhelp\nif\nno\nparams\nare\ngiven.\n\n##\nRunning\nTests\n\nTesting\nfirst\nrequires\n[building\nSpark](#building-spark).\nOnce\nSpark\nis\nbuilt,\ntests\ncan\nbe\nrun\nusing:\n\n\n\n\n\n./dev/run-tests\n\nPlease\nsee\nthe\nguidance\non\nhow\nto\n[run\ntests\nfor\na\nmodule,\nor\nindividual\ntests](https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools).\n\n##\nA\nNote\nAbout\nHadoop\nVersions\n\nSpark\nuses\nthe\nHadoop\ncore\nlibrary\nto\ntalk\nto\nHDFS\nand\nother\nHadoop-supported\nstorage\nsystems.\nBecause\nthe\nprotocols\nhave\nchanged\nin\ndifferent\nversions\nof\nHadoop,\nyou\nmust\nbuild\nSpark\nagainst\nthe\nsame\nversion\nthat\nyour\ncluster\nruns.\n\nPlease\nrefer\nto\nthe\nbuild\ndocumentation\nat\n[\"Specifying\nthe\nHadoop\nVersion\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version)\nfor\ndetailed\nguidance\non\nbuilding\nfor\na\nparticular\ndistribution\nof\nHadoop,\nincluding\nbuilding\nfor\nparticular\nHive\nand\nHive\nThriftserver\ndistributions.\n\n##\nConfiguration\n\nPlease\nrefer\nto\nthe\n[Configuration\nGuide](http://spark.apache.org/docs/latest/configuration.html)\nin\nthe\nonline\ndocumentation\nfor\nan\noverview\non\nhow\nto\nconfigure\nSpark.\n"
     ]
    }
   ],
   "source": [
    "# Example: lambda functions  \n",
    "words = rdd.flatMap(lambda lines: lines.split(\" \"))\n",
    "\n",
    "for w in words.collect():\n",
    "  print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "222ddcdd-271e-4703-9cd9-e2ed3bdf92aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#: 1\nApache: 1\nSpark: 13\n: 67\nis: 6\nIt: 2\nprovides: 1\nhigh-level: 1\nAPIs: 1\nin: 5\nScala,: 1\nJava,: 1\nan: 3\noptimized: 1\nengine: 1\nsupports: 2\ncomputation: 1\nanalysis.: 1\nset: 2\nof: 5\ntools: 1\nSQL: 2\nMLlib: 1\nmachine: 1\nlearning,: 1\nGraphX: 1\ngraph: 1\nprocessing,: 1\nDocumentation: 1\nlatest: 1\nprogramming: 1\nguide,: 1\n[project: 2\nREADME: 1\nonly: 1\nbasic: 1\ninstructions.: 1\nBuilding: 1\nusing: 2\n[Apache: 1\nrun:: 1\ndo: 2\nthis: 1\ndownloaded: 1\ndocumentation: 3\nproject: 1\nsite,: 1\nat: 2\nSpark\"](http://spark.apache.org/docs/latest/building-spark.html).: 1\nInteractive: 2\nShell: 2\nThe: 1\nway: 1\nstart: 1\nTry: 1\nfollowing: 2\n1000:: 2\nscala>: 1\n1000).count(): 1\nPython: 2\nAlternatively,: 1\nuse: 3\nAnd: 1\nrun: 7\nExample: 1\nseveral: 1\nprograms: 2\nthem,: 1\n`./bin/run-example: 1\n[params]`.: 1\nexample:: 1\n./bin/run-example: 2\nSparkPi: 2\nvariable: 1\nwhen: 1\nexamples: 2\nspark://: 1\nURL,: 1\nYARN,: 1\n\"local\": 1\nlocally: 2\nN: 1\nabbreviated: 1\nclass: 2\nname: 1\npackage.: 1\ninstance:: 1\nprint: 1\nusage: 1\nhelp: 1\nno: 1\nparams: 1\nare: 1\nTesting: 1\nSpark](#building-spark).: 1\nOnce: 1\nbuilt,: 1\ntests: 2\nusing:: 1\n./dev/run-tests: 1\nPlease: 3\nguidance: 2\nmodule,: 1\nindividual: 1\nNote: 1\nAbout: 1\nuses: 1\nlibrary: 1\nHDFS: 1\nother: 1\nHadoop-supported: 1\nstorage: 1\nsystems.: 1\nBecause: 1\nhave: 1\nchanged: 1\ndifferent: 1\nversions: 1\nHadoop,: 2\nmust: 1\nagainst: 1\nversion: 1\nrefer: 2\nparticular: 2\ndistribution: 1\nHive: 2\nThriftserver: 1\ndistributions.: 1\n[Configuration: 1\nGuide](http://spark.apache.org/docs/latest/configuration.html): 1\nonline: 1\noverview: 1\nconfigure: 1\nSpark.: 1\na: 8\nfast: 1\nand: 10\ngeneral: 2\ncluster: 2\ncomputing: 1\nsystem: 1\nfor: 11\nBig: 1\nData.: 1\nPython,: 2\nR,: 1\nthat: 2\ngraphs: 1\ndata: 1\nalso: 4\nrich: 1\nhigher-level: 1\nincluding: 3\nDataFrames,: 1\nStreaming: 1\nstream: 1\nprocessing.: 1\n<http://spark.apache.org/>: 1\n##: 8\nOnline: 1\nYou: 3\ncan: 6\nfind: 1\nthe: 21\ndocumentation,: 1\non: 5\nweb: 1\npage](http://spark.apache.org/documentation.html): 1\nwiki](https://cwiki.apache.org/confluence/display/SPARK).: 1\nThis: 2\nfile: 1\ncontains: 1\nsetup: 1\nbuilt: 1\nMaven](http://maven.apache.org/).: 1\nTo: 2\nbuild: 3\nits: 1\nexample: 3\nprograms,: 1\nbuild/mvn: 1\n-DskipTests: 1\nclean: 1\npackage: 1\n(You: 1\nnot: 1\nneed: 1\nto: 14\nif: 4\nyou: 4\npre-built: 1\npackage.): 1\nMore: 1\ndetailed: 2\navailable: 1\nfrom: 1\n[\"Building: 1\nScala: 2\neasiest: 1\nthrough: 1\nshell:: 2\n./bin/spark-shell: 1\ncommand,: 2\nwhich: 2\nshould: 2\nreturn: 2\nsc.parallelize(1: 1\nprefer: 1\n./bin/pyspark: 1\n>>>: 1\nsc.parallelize(range(1000)).count(): 1\nPrograms: 1\ncomes: 1\nwith: 3\nsample: 1\n`examples`: 2\ndirectory.: 1\none: 2\n<class>: 1\nFor: 2\nwill: 1\nPi: 1\nlocally.: 1\nMASTER: 1\nenvironment: 1\nrunning: 1\nsubmit: 1\ncluster.: 1\nbe: 2\nmesos://: 1\nor: 3\n\"yarn\": 1\nthread,: 1\n\"local[N]\": 1\nthreads.: 1\nMASTER=spark://host:7077: 1\nMany: 1\ngiven.: 1\nRunning: 1\nTests: 1\nfirst: 1\nrequires: 1\n[building: 1\nsee: 1\nhow: 2\n[run: 1\ntests](https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools).: 1\nA: 1\nHadoop: 3\nVersions: 1\ncore: 1\ntalk: 1\nprotocols: 1\nsame: 1\nyour: 1\nruns.: 1\n[\"Specifying: 1\nVersion\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version): 1\nbuilding: 2\nConfiguration: 1\n"
     ]
    }
   ],
   "source": [
    "# Take the previous function and\n",
    "# 1. count the occurence of each word\n",
    "# Map each word to a key-value pair (word, 1)\n",
    "word_pairs = words.map(lambda word: (word, 1))\n",
    "\n",
    "# Reduce by key to count the occurrences of each word\n",
    "word_counts = word_pairs.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Collect and print the word counts\n",
    "for word, count in word_counts.collect():\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91d9a83d-2ecd-4f59-80b6-dae5646d91c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[6]: ['#',\n 'apache',\n 'spark',\n '',\n 'spark',\n 'is',\n 'a',\n 'fast',\n 'and',\n 'general',\n 'cluster',\n 'computing',\n 'system',\n 'for',\n 'big',\n 'data.',\n 'it',\n 'provides',\n 'high-level',\n 'apis',\n 'in',\n 'scala,',\n 'java,',\n 'python,',\n 'and',\n 'r,',\n 'and',\n 'an',\n 'optimized',\n 'engine',\n 'that',\n 'supports',\n 'general',\n 'computation',\n 'graphs',\n 'for',\n 'data',\n 'analysis.',\n 'it',\n 'also',\n 'supports',\n 'a',\n 'rich',\n 'set',\n 'of',\n 'higher-level',\n 'tools',\n 'including',\n 'spark',\n 'sql',\n 'for',\n 'sql',\n 'and',\n 'dataframes,',\n 'mllib',\n 'for',\n 'machine',\n 'learning,',\n 'graphx',\n 'for',\n 'graph',\n 'processing,',\n 'and',\n 'spark',\n 'streaming',\n 'for',\n 'stream',\n 'processing.',\n '',\n '<http://spark.apache.org/>',\n '',\n '',\n '##',\n 'online',\n 'documentation',\n '',\n 'you',\n 'can',\n 'find',\n 'the',\n 'latest',\n 'spark',\n 'documentation,',\n 'including',\n 'a',\n 'programming',\n 'guide,',\n 'on',\n 'the',\n '[project',\n 'web',\n 'page](http://spark.apache.org/documentation.html)',\n 'and',\n '[project',\n 'wiki](https://cwiki.apache.org/confluence/display/spark).',\n 'this',\n 'readme',\n 'file',\n 'only',\n 'contains',\n 'basic',\n 'setup',\n 'instructions.',\n '',\n '##',\n 'building',\n 'spark',\n '',\n 'spark',\n 'is',\n 'built',\n 'using',\n '[apache',\n 'maven](http://maven.apache.org/).',\n 'to',\n 'build',\n 'spark',\n 'and',\n 'its',\n 'example',\n 'programs,',\n 'run:',\n '',\n '',\n '',\n '',\n '',\n 'build/mvn',\n '-dskiptests',\n 'clean',\n 'package',\n '',\n '(you',\n 'do',\n 'not',\n 'need',\n 'to',\n 'do',\n 'this',\n 'if',\n 'you',\n 'downloaded',\n 'a',\n 'pre-built',\n 'package.)',\n 'more',\n 'detailed',\n 'documentation',\n 'is',\n 'available',\n 'from',\n 'the',\n 'project',\n 'site,',\n 'at',\n '[\"building',\n 'spark\"](http://spark.apache.org/docs/latest/building-spark.html).',\n '',\n '##',\n 'interactive',\n 'scala',\n 'shell',\n '',\n 'the',\n 'easiest',\n 'way',\n 'to',\n 'start',\n 'using',\n 'spark',\n 'is',\n 'through',\n 'the',\n 'scala',\n 'shell:',\n '',\n '',\n '',\n '',\n '',\n './bin/spark-shell',\n '',\n 'try',\n 'the',\n 'following',\n 'command,',\n 'which',\n 'should',\n 'return',\n '1000:',\n '',\n '',\n '',\n '',\n '',\n 'scala>',\n 'sc.parallelize(1',\n 'to',\n '1000).count()',\n '',\n '##',\n 'interactive',\n 'python',\n 'shell',\n '',\n 'alternatively,',\n 'if',\n 'you',\n 'prefer',\n 'python,',\n 'you',\n 'can',\n 'use',\n 'the',\n 'python',\n 'shell:',\n '',\n '',\n '',\n '',\n '',\n './bin/pyspark',\n '',\n 'and',\n 'run',\n 'the',\n 'following',\n 'command,',\n 'which',\n 'should',\n 'also',\n 'return',\n '1000:',\n '',\n '',\n '',\n '',\n '',\n '>>>',\n 'sc.parallelize(range(1000)).count()',\n '',\n '##',\n 'example',\n 'programs',\n '',\n 'spark',\n 'also',\n 'comes',\n 'with',\n 'several',\n 'sample',\n 'programs',\n 'in',\n 'the',\n '`examples`',\n 'directory.',\n 'to',\n 'run',\n 'one',\n 'of',\n 'them,',\n 'use',\n '`./bin/run-example',\n '<class>',\n '[params]`.',\n 'for',\n 'example:',\n '',\n '',\n '',\n '',\n '',\n './bin/run-example',\n 'sparkpi',\n '',\n 'will',\n 'run',\n 'the',\n 'pi',\n 'example',\n 'locally.',\n '',\n 'you',\n 'can',\n 'set',\n 'the',\n 'master',\n 'environment',\n 'variable',\n 'when',\n 'running',\n 'examples',\n 'to',\n 'submit',\n 'examples',\n 'to',\n 'a',\n 'cluster.',\n 'this',\n 'can',\n 'be',\n 'a',\n 'mesos://',\n 'or',\n 'spark://',\n 'url,',\n '\"yarn\"',\n 'to',\n 'run',\n 'on',\n 'yarn,',\n 'and',\n '\"local\"',\n 'to',\n 'run',\n 'locally',\n 'with',\n 'one',\n 'thread,',\n 'or',\n '\"local[n]\"',\n 'to',\n 'run',\n 'locally',\n 'with',\n 'n',\n 'threads.',\n 'you',\n 'can',\n 'also',\n 'use',\n 'an',\n 'abbreviated',\n 'class',\n 'name',\n 'if',\n 'the',\n 'class',\n 'is',\n 'in',\n 'the',\n '`examples`',\n 'package.',\n 'for',\n 'instance:',\n '',\n '',\n '',\n '',\n '',\n 'master=spark://host:7077',\n './bin/run-example',\n 'sparkpi',\n '',\n 'many',\n 'of',\n 'the',\n 'example',\n 'programs',\n 'print',\n 'usage',\n 'help',\n 'if',\n 'no',\n 'params',\n 'are',\n 'given.',\n '',\n '##',\n 'running',\n 'tests',\n '',\n 'testing',\n 'first',\n 'requires',\n '[building',\n 'spark](#building-spark).',\n 'once',\n 'spark',\n 'is',\n 'built,',\n 'tests',\n 'can',\n 'be',\n 'run',\n 'using:',\n '',\n '',\n '',\n '',\n '',\n './dev/run-tests',\n '',\n 'please',\n 'see',\n 'the',\n 'guidance',\n 'on',\n 'how',\n 'to',\n '[run',\n 'tests',\n 'for',\n 'a',\n 'module,',\n 'or',\n 'individual',\n 'tests](https://cwiki.apache.org/confluence/display/spark/useful+developer+tools).',\n '',\n '##',\n 'a',\n 'note',\n 'about',\n 'hadoop',\n 'versions',\n '',\n 'spark',\n 'uses',\n 'the',\n 'hadoop',\n 'core',\n 'library',\n 'to',\n 'talk',\n 'to',\n 'hdfs',\n 'and',\n 'other',\n 'hadoop-supported',\n 'storage',\n 'systems.',\n 'because',\n 'the',\n 'protocols',\n 'have',\n 'changed',\n 'in',\n 'different',\n 'versions',\n 'of',\n 'hadoop,',\n 'you',\n 'must',\n 'build',\n 'spark',\n 'against',\n 'the',\n 'same',\n 'version',\n 'that',\n 'your',\n 'cluster',\n 'runs.',\n '',\n 'please',\n 'refer',\n 'to',\n 'the',\n 'build',\n 'documentation',\n 'at',\n '[\"specifying',\n 'the',\n 'hadoop',\n 'version\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version)',\n 'for',\n 'detailed',\n 'guidance',\n 'on',\n 'building',\n 'for',\n 'a',\n 'particular',\n 'distribution',\n 'of',\n 'hadoop,',\n 'including',\n 'building',\n 'for',\n 'particular',\n 'hive',\n 'and',\n 'hive',\n 'thriftserver',\n 'distributions.',\n '',\n '##',\n 'configuration',\n '',\n 'please',\n 'refer',\n 'to',\n 'the',\n '[configuration',\n 'guide](http://spark.apache.org/docs/latest/configuration.html)',\n 'in',\n 'the',\n 'online',\n 'documentation',\n 'for',\n 'an',\n 'overview',\n 'on',\n 'how',\n 'to',\n 'configure',\n 'spark.']"
     ]
    }
   ],
   "source": [
    "# 2. change all capital letters to lower case\n",
    "# Convert each word to lowercase\n",
    "words = rdd.flatMap(lambda lines: lines.split(\" \")).map(lambda word: word.lower())\n",
    "words.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a41d715-efd1-48be-8c6d-f822e71fb4bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[7]: ['#',\n 'apache',\n 'spark',\n '',\n 'spark',\n 'is',\n 'a',\n 'fast',\n 'general',\n 'cluster',\n 'computing',\n 'system',\n 'for',\n 'big',\n 'data.',\n 'it',\n 'provides',\n 'high-level',\n 'apis',\n 'scala,',\n 'java,',\n 'python,',\n 'r,',\n 'optimized',\n 'engine',\n 'that',\n 'supports',\n 'general',\n 'computation',\n 'graphs',\n 'for',\n 'data',\n 'analysis.',\n 'it',\n 'also',\n 'supports',\n 'a',\n 'rich',\n 'set',\n 'of',\n 'higher-level',\n 'tools',\n 'including',\n 'spark',\n 'sql',\n 'for',\n 'sql',\n 'dataframes,',\n 'mllib',\n 'for',\n 'machine',\n 'learning,',\n 'graphx',\n 'for',\n 'graph',\n 'processing,',\n 'spark',\n 'streaming',\n 'for',\n 'stream',\n 'processing.',\n '',\n '<http://spark.apache.org/>',\n '',\n '',\n '##',\n 'online',\n 'documentation',\n '',\n 'you',\n 'can',\n 'find',\n 'latest',\n 'spark',\n 'documentation,',\n 'including',\n 'a',\n 'programming',\n 'guide,',\n 'on',\n '[project',\n 'web',\n 'page](http://spark.apache.org/documentation.html)',\n '[project',\n 'wiki](https://cwiki.apache.org/confluence/display/spark).',\n 'this',\n 'readme',\n 'file',\n 'only',\n 'contains',\n 'basic',\n 'setup',\n 'instructions.',\n '',\n '##',\n 'building',\n 'spark',\n '',\n 'spark',\n 'is',\n 'built',\n 'using',\n '[apache',\n 'maven](http://maven.apache.org/).',\n 'build',\n 'spark',\n 'its',\n 'example',\n 'programs,',\n 'run:',\n '',\n '',\n '',\n '',\n '',\n 'build/mvn',\n '-dskiptests',\n 'clean',\n 'package',\n '',\n '(you',\n 'do',\n 'not',\n 'need',\n 'do',\n 'this',\n 'if',\n 'you',\n 'downloaded',\n 'a',\n 'pre-built',\n 'package.)',\n 'more',\n 'detailed',\n 'documentation',\n 'is',\n 'available',\n 'from',\n 'project',\n 'site,',\n '[\"building',\n 'spark\"](http://spark.apache.org/docs/latest/building-spark.html).',\n '',\n '##',\n 'interactive',\n 'scala',\n 'shell',\n '',\n 'easiest',\n 'way',\n 'start',\n 'using',\n 'spark',\n 'is',\n 'through',\n 'scala',\n 'shell:',\n '',\n '',\n '',\n '',\n '',\n './bin/spark-shell',\n '',\n 'try',\n 'following',\n 'command,',\n 'which',\n 'should',\n 'return',\n '1000:',\n '',\n '',\n '',\n '',\n '',\n 'scala>',\n 'sc.parallelize(1',\n '1000).count()',\n '',\n '##',\n 'interactive',\n 'python',\n 'shell',\n '',\n 'alternatively,',\n 'if',\n 'you',\n 'prefer',\n 'python,',\n 'you',\n 'can',\n 'use',\n 'python',\n 'shell:',\n '',\n '',\n '',\n '',\n '',\n './bin/pyspark',\n '',\n 'run',\n 'following',\n 'command,',\n 'which',\n 'should',\n 'also',\n 'return',\n '1000:',\n '',\n '',\n '',\n '',\n '',\n '>>>',\n 'sc.parallelize(range(1000)).count()',\n '',\n '##',\n 'example',\n 'programs',\n '',\n 'spark',\n 'also',\n 'comes',\n 'with',\n 'several',\n 'sample',\n 'programs',\n '`examples`',\n 'directory.',\n 'run',\n 'one',\n 'of',\n 'them,',\n 'use',\n '`./bin/run-example',\n '<class>',\n '[params]`.',\n 'for',\n 'example:',\n '',\n '',\n '',\n '',\n '',\n './bin/run-example',\n 'sparkpi',\n '',\n 'will',\n 'run',\n 'pi',\n 'example',\n 'locally.',\n '',\n 'you',\n 'can',\n 'set',\n 'master',\n 'environment',\n 'variable',\n 'when',\n 'running',\n 'examples',\n 'submit',\n 'examples',\n 'a',\n 'cluster.',\n 'this',\n 'can',\n 'be',\n 'a',\n 'mesos://',\n 'or',\n 'spark://',\n 'url,',\n '\"yarn\"',\n 'run',\n 'on',\n 'yarn,',\n '\"local\"',\n 'run',\n 'locally',\n 'with',\n 'one',\n 'thread,',\n 'or',\n '\"local[n]\"',\n 'run',\n 'locally',\n 'with',\n 'n',\n 'threads.',\n 'you',\n 'can',\n 'also',\n 'use',\n 'abbreviated',\n 'class',\n 'name',\n 'if',\n 'class',\n 'is',\n '`examples`',\n 'package.',\n 'for',\n 'instance:',\n '',\n '',\n '',\n '',\n '',\n 'master=spark://host:7077',\n './bin/run-example',\n 'sparkpi',\n '',\n 'many',\n 'of',\n 'example',\n 'programs',\n 'print',\n 'usage',\n 'help',\n 'if',\n 'no',\n 'params',\n 'are',\n 'given.',\n '',\n '##',\n 'running',\n 'tests',\n '',\n 'testing',\n 'first',\n 'requires',\n '[building',\n 'spark](#building-spark).',\n 'once',\n 'spark',\n 'is',\n 'built,',\n 'tests',\n 'can',\n 'be',\n 'run',\n 'using:',\n '',\n '',\n '',\n '',\n '',\n './dev/run-tests',\n '',\n 'please',\n 'see',\n 'guidance',\n 'on',\n 'how',\n '[run',\n 'tests',\n 'for',\n 'a',\n 'module,',\n 'or',\n 'individual',\n 'tests](https://cwiki.apache.org/confluence/display/spark/useful+developer+tools).',\n '',\n '##',\n 'a',\n 'note',\n 'about',\n 'hadoop',\n 'versions',\n '',\n 'spark',\n 'uses',\n 'hadoop',\n 'core',\n 'library',\n 'talk',\n 'hdfs',\n 'other',\n 'hadoop-supported',\n 'storage',\n 'systems.',\n 'because',\n 'protocols',\n 'have',\n 'changed',\n 'different',\n 'versions',\n 'of',\n 'hadoop,',\n 'you',\n 'must',\n 'build',\n 'spark',\n 'against',\n 'same',\n 'version',\n 'that',\n 'your',\n 'cluster',\n 'runs.',\n '',\n 'please',\n 'refer',\n 'build',\n 'documentation',\n '[\"specifying',\n 'hadoop',\n 'version\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version)',\n 'for',\n 'detailed',\n 'guidance',\n 'on',\n 'building',\n 'for',\n 'a',\n 'particular',\n 'distribution',\n 'of',\n 'hadoop,',\n 'including',\n 'building',\n 'for',\n 'particular',\n 'hive',\n 'hive',\n 'thriftserver',\n 'distributions.',\n '',\n '##',\n 'configuration',\n '',\n 'please',\n 'refer',\n '[configuration',\n 'guide](http://spark.apache.org/docs/latest/configuration.html)',\n 'online',\n 'documentation',\n 'for',\n 'overview',\n 'on',\n 'how',\n 'configure',\n 'spark.']"
     ]
    }
   ],
   "source": [
    "# 3. eliminate stopwords \n",
    "stop_words = ['and', 'to', 'in', 'at', 'the', 'an']\n",
    "\n",
    "words = rdd.flatMap(lambda lines: lines.split(\" \")) \\\n",
    "           .map(lambda word: word.lower()) \\\n",
    "           .filter(lambda word: word not in stop_words)\n",
    "words.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19b3b435-df3e-4181-8eb4-453cc354e576",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[8]: ['',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '\"local\"',\n '\"local[n]\"',\n '\"yarn\"',\n '#',\n '##',\n '##',\n '##',\n '##',\n '##',\n '##',\n '##',\n '##',\n '(you',\n '-dskiptests',\n './bin/pyspark',\n './bin/run-example',\n './bin/run-example',\n './bin/spark-shell',\n './dev/run-tests',\n '1000).count()',\n '1000:',\n '1000:',\n '<class>',\n '<http://spark.apache.org/>',\n '>>>',\n '[\"building',\n '[\"specifying',\n '[apache',\n '[building',\n '[configuration',\n '[params]`.',\n '[project',\n '[project',\n '[run',\n '`./bin/run-example',\n '`examples`',\n '`examples`',\n 'a',\n 'a',\n 'a',\n 'a',\n 'a',\n 'a',\n 'a',\n 'a',\n 'a',\n 'abbreviated',\n 'about',\n 'against',\n 'also',\n 'also',\n 'also',\n 'also',\n 'alternatively,',\n 'analysis.',\n 'apache',\n 'apis',\n 'are',\n 'available',\n 'basic',\n 'be',\n 'be',\n 'because',\n 'big',\n 'build',\n 'build',\n 'build',\n 'build/mvn',\n 'building',\n 'building',\n 'building',\n 'built',\n 'built,',\n 'can',\n 'can',\n 'can',\n 'can',\n 'can',\n 'can',\n 'changed',\n 'class',\n 'class',\n 'clean',\n 'cluster',\n 'cluster',\n 'cluster.',\n 'comes',\n 'command,',\n 'command,',\n 'computation',\n 'computing',\n 'configuration',\n 'configure',\n 'contains',\n 'core',\n 'data',\n 'data.',\n 'dataframes,',\n 'detailed',\n 'detailed',\n 'different',\n 'directory.',\n 'distribution',\n 'distributions.',\n 'do',\n 'do',\n 'documentation',\n 'documentation',\n 'documentation',\n 'documentation',\n 'documentation,',\n 'downloaded',\n 'easiest',\n 'engine',\n 'environment',\n 'example',\n 'example',\n 'example',\n 'example',\n 'example:',\n 'examples',\n 'examples',\n 'fast',\n 'file',\n 'find',\n 'first',\n 'following',\n 'following',\n 'for',\n 'for',\n 'for',\n 'for',\n 'for',\n 'for',\n 'for',\n 'for',\n 'for',\n 'for',\n 'for',\n 'for',\n 'for',\n 'from',\n 'general',\n 'general',\n 'given.',\n 'graph',\n 'graphs',\n 'graphx',\n 'guidance',\n 'guidance',\n 'guide,',\n 'guide](http://spark.apache.org/docs/latest/configuration.html)',\n 'hadoop',\n 'hadoop',\n 'hadoop',\n 'hadoop,',\n 'hadoop,',\n 'hadoop-supported',\n 'have',\n 'hdfs',\n 'help',\n 'high-level',\n 'higher-level',\n 'hive',\n 'hive',\n 'how',\n 'how',\n 'if',\n 'if',\n 'if',\n 'if',\n 'including',\n 'including',\n 'including',\n 'individual',\n 'instance:',\n 'instructions.',\n 'interactive',\n 'interactive',\n 'is',\n 'is',\n 'is',\n 'is',\n 'is',\n 'is',\n 'it',\n 'it',\n 'its',\n 'java,',\n 'latest',\n 'learning,',\n 'library',\n 'locally',\n 'locally',\n 'locally.',\n 'machine',\n 'many',\n 'master',\n 'master=spark://host:7077',\n 'maven](http://maven.apache.org/).',\n 'mesos://',\n 'mllib',\n 'module,',\n 'more',\n 'must',\n 'n',\n 'name',\n 'need',\n 'no',\n 'not',\n 'note',\n 'of',\n 'of',\n 'of',\n 'of',\n 'of',\n 'on',\n 'on',\n 'on',\n 'on',\n 'on',\n 'once',\n 'one',\n 'one',\n 'online',\n 'online',\n 'only',\n 'optimized',\n 'or',\n 'or',\n 'or',\n 'other',\n 'overview',\n 'package',\n 'package.',\n 'package.)',\n 'page](http://spark.apache.org/documentation.html)',\n 'params',\n 'particular',\n 'particular',\n 'pi',\n 'please',\n 'please',\n 'please',\n 'pre-built',\n 'prefer',\n 'print',\n 'processing,',\n 'processing.',\n 'programming',\n 'programs',\n 'programs',\n 'programs',\n 'programs,',\n 'project',\n 'protocols',\n 'provides',\n 'python',\n 'python',\n 'python,',\n 'python,',\n 'r,',\n 'readme',\n 'refer',\n 'refer',\n 'requires',\n 'return',\n 'return',\n 'rich',\n 'run',\n 'run',\n 'run',\n 'run',\n 'run',\n 'run',\n 'run',\n 'run:',\n 'running',\n 'running',\n 'runs.',\n 'same',\n 'sample',\n 'sc.parallelize(1',\n 'sc.parallelize(range(1000)).count()',\n 'scala',\n 'scala',\n 'scala,',\n 'scala>',\n 'see',\n 'set',\n 'set',\n 'setup',\n 'several',\n 'shell',\n 'shell',\n 'shell:',\n 'shell:',\n 'should',\n 'should',\n 'site,',\n 'spark',\n 'spark',\n 'spark',\n 'spark',\n 'spark',\n 'spark',\n 'spark',\n 'spark',\n 'spark',\n 'spark',\n 'spark',\n 'spark',\n 'spark',\n 'spark\"](http://spark.apache.org/docs/latest/building-spark.html).',\n 'spark.',\n 'spark://',\n 'spark](#building-spark).',\n 'sparkpi',\n 'sparkpi',\n 'sql',\n 'sql',\n 'start',\n 'storage',\n 'stream',\n 'streaming',\n 'submit',\n 'supports',\n 'supports',\n 'system',\n 'systems.',\n 'talk',\n 'testing',\n 'tests',\n 'tests',\n 'tests',\n 'tests](https://cwiki.apache.org/confluence/display/spark/useful+developer+tools).',\n 'that',\n 'that',\n 'them,',\n 'this',\n 'this',\n 'this',\n 'thread,',\n 'threads.',\n 'thriftserver',\n 'through',\n 'tools',\n 'try',\n 'url,',\n 'usage',\n 'use',\n 'use',\n 'use',\n 'uses',\n 'using',\n 'using',\n 'using:',\n 'variable',\n 'version',\n 'version\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version)',\n 'versions',\n 'versions',\n 'way',\n 'web',\n 'when',\n 'which',\n 'which',\n 'wiki](https://cwiki.apache.org/confluence/display/spark).',\n 'will',\n 'with',\n 'with',\n 'with',\n 'yarn,',\n 'you',\n 'you',\n 'you',\n 'you',\n 'you',\n 'you',\n 'you',\n 'your']"
     ]
    }
   ],
   "source": [
    "# 4. sort in alphabetical order\n",
    "words = words.sortBy(lambda word: word)\n",
    "words.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "103ffb91-f715-4b9d-b6ac-8c47fb699233",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 5. sort from most to least frequent word\n",
    "# Sort by frequency (most to least)\n",
    "word_counts = word_pairs.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Sort by count in descending order\n",
    "sorted_word_counts = word_counts.sortBy(lambda x: x[1], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4eaa9bb-2882-41fc-b676-2c24844e80bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[10]: ['',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n 'local',\n 'localn',\n 'yarn',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n 'you',\n 'dskiptests',\n 'binpyspark',\n 'binrunexample',\n 'binrunexample',\n 'binsparkshell',\n 'devruntests',\n '1000count',\n '1000',\n '1000',\n 'class',\n 'httpsparkapacheorg',\n '',\n 'building',\n 'specifying',\n 'apache',\n 'building',\n 'configuration',\n 'params',\n 'project',\n 'project',\n 'run',\n 'binrunexample',\n 'examples',\n 'examples',\n 'a',\n 'a',\n 'a',\n 'a',\n 'a',\n 'a',\n 'a',\n 'a',\n 'a',\n 'abbreviated',\n 'about',\n 'against',\n 'also',\n 'also',\n 'also',\n 'also',\n 'alternatively',\n 'analysis',\n 'apache',\n 'apis',\n 'are',\n 'available',\n 'basic',\n 'be',\n 'be',\n 'because',\n 'big',\n 'build',\n 'build',\n 'build',\n 'buildmvn',\n 'building',\n 'building',\n 'building',\n 'built',\n 'built',\n 'can',\n 'can',\n 'can',\n 'can',\n 'can',\n 'can',\n 'changed',\n 'class',\n 'class',\n 'clean',\n 'cluster',\n 'cluster',\n 'cluster',\n 'comes',\n 'command',\n 'command',\n 'computation',\n 'computing',\n 'configuration',\n 'configure',\n 'contains',\n 'core',\n 'data',\n 'data',\n 'dataframes',\n 'detailed',\n 'detailed',\n 'different',\n 'directory',\n 'distribution',\n 'distributions',\n 'do',\n 'do',\n 'documentation',\n 'documentation',\n 'documentation',\n 'documentation',\n 'documentation',\n 'downloaded',\n 'easiest',\n 'engine',\n 'environment',\n 'example',\n 'example',\n 'example',\n 'example',\n 'example',\n 'examples',\n 'examples',\n 'fast',\n 'file',\n 'find',\n 'first',\n 'following',\n 'following',\n 'for',\n 'for',\n 'for',\n 'for',\n 'for',\n 'for',\n 'for',\n 'for',\n 'for',\n 'for',\n 'for',\n 'for',\n 'for',\n 'from',\n 'general',\n 'general',\n 'given',\n 'graph',\n 'graphs',\n 'graphx',\n 'guidance',\n 'guidance',\n 'guide',\n 'guidehttpsparkapacheorgdocslatestconfigurationhtml',\n 'hadoop',\n 'hadoop',\n 'hadoop',\n 'hadoop',\n 'hadoop',\n 'hadoopsupported',\n 'have',\n 'hdfs',\n 'help',\n 'highlevel',\n 'higherlevel',\n 'hive',\n 'hive',\n 'how',\n 'how',\n 'if',\n 'if',\n 'if',\n 'if',\n 'including',\n 'including',\n 'including',\n 'individual',\n 'instance',\n 'instructions',\n 'interactive',\n 'interactive',\n 'is',\n 'is',\n 'is',\n 'is',\n 'is',\n 'is',\n 'it',\n 'it',\n 'its',\n 'java',\n 'latest',\n 'learning',\n 'library',\n 'locally',\n 'locally',\n 'locally',\n 'machine',\n 'many',\n 'master',\n 'mastersparkhost7077',\n 'mavenhttpmavenapacheorg',\n 'mesos',\n 'mllib',\n 'module',\n 'more',\n 'must',\n 'n',\n 'name',\n 'need',\n 'no',\n 'not',\n 'note',\n 'of',\n 'of',\n 'of',\n 'of',\n 'of',\n 'on',\n 'on',\n 'on',\n 'on',\n 'on',\n 'once',\n 'one',\n 'one',\n 'online',\n 'online',\n 'only',\n 'optimized',\n 'or',\n 'or',\n 'or',\n 'other',\n 'overview',\n 'package',\n 'package',\n 'package',\n 'pagehttpsparkapacheorgdocumentationhtml',\n 'params',\n 'particular',\n 'particular',\n 'pi',\n 'please',\n 'please',\n 'please',\n 'prebuilt',\n 'prefer',\n 'print',\n 'processing',\n 'processing',\n 'programming',\n 'programs',\n 'programs',\n 'programs',\n 'programs',\n 'project',\n 'protocols',\n 'provides',\n 'python',\n 'python',\n 'python',\n 'python',\n 'r',\n 'readme',\n 'refer',\n 'refer',\n 'requires',\n 'return',\n 'return',\n 'rich',\n 'run',\n 'run',\n 'run',\n 'run',\n 'run',\n 'run',\n 'run',\n 'run',\n 'running',\n 'running',\n 'runs',\n 'same',\n 'sample',\n 'scparallelize1',\n 'scparallelizerange1000count',\n 'scala',\n 'scala',\n 'scala',\n 'scala',\n 'see',\n 'set',\n 'set',\n 'setup',\n 'several',\n 'shell',\n 'shell',\n 'shell',\n 'shell',\n 'should',\n 'should',\n 'site',\n 'spark',\n 'spark',\n 'spark',\n 'spark',\n 'spark',\n 'spark',\n 'spark',\n 'spark',\n 'spark',\n 'spark',\n 'spark',\n 'spark',\n 'spark',\n 'sparkhttpsparkapacheorgdocslatestbuildingsparkhtml',\n 'spark',\n 'spark',\n 'sparkbuildingspark',\n 'sparkpi',\n 'sparkpi',\n 'sql',\n 'sql',\n 'start',\n 'storage',\n 'stream',\n 'streaming',\n 'submit',\n 'supports',\n 'supports',\n 'system',\n 'systems',\n 'talk',\n 'testing',\n 'tests',\n 'tests',\n 'tests',\n 'testshttpscwikiapacheorgconfluencedisplaysparkusefuldevelopertools',\n 'that',\n 'that',\n 'them',\n 'this',\n 'this',\n 'this',\n 'thread',\n 'threads',\n 'thriftserver',\n 'through',\n 'tools',\n 'try',\n 'url',\n 'usage',\n 'use',\n 'use',\n 'use',\n 'uses',\n 'using',\n 'using',\n 'using',\n 'variable',\n 'version',\n 'versionhttpsparkapacheorgdocslatestbuildingsparkhtmlspecifyingthehadoopversion',\n 'versions',\n 'versions',\n 'way',\n 'web',\n 'when',\n 'which',\n 'which',\n 'wikihttpscwikiapacheorgconfluencedisplayspark',\n 'will',\n 'with',\n 'with',\n 'with',\n 'yarn',\n 'you',\n 'you',\n 'you',\n 'you',\n 'you',\n 'you',\n 'you',\n 'your']"
     ]
    }
   ],
   "source": [
    "# 6.** remove punctuations \n",
    "import string\n",
    "\n",
    "# Remove punctuation from each word\n",
    "words = words.map(lambda word: word.translate(str.maketrans('', '', string.punctuation)))\n",
    "words.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e683e60a-c072-4ace-8ceb-0f615bb9bbcc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. What does it do?"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Lab1_RDD_practice",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
